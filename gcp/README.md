# gcp/

Survey response ETL pipeline that receives Qualtrics Web Service task payloads via a Cloud Run function, validates them with Pydantic, and writes them to BigQuery as explicit typed columns.

## Directory structure

```
gcp/
  cloud_run_functions/
    run_qualtrics_scheduling/     # Cloud Run function source
      configs/
        gcp_config.yaml           # GCP project, BigQuery dataset/table names
        qualtrics_config.yaml     # Qualtrics API endpoints
      models/
        qualtrics.py              # WebServicePayload, QID_MAP, scale constants
        participant.py            # ParticipantData, phone normalization
      utils/
        validation_utils.py       # Payload parsing, participant extraction
        qualtrics_utils.py        # API fetch (legacy), follow-up URL builder
      main.py                     # Cloud Function entry point
      requirements.txt            # Generated by Poetry at deploy time
  shared/
    utils/
      bq_schemas.py               # Schema generation from Pydantic models
      config_loader.py            # YAML loader with Pydantic validation
      config_models.py            # AppConfig and nested config models
      gcp_utils.py                # BigQuery insert operations
  deploy/
    functions.yaml                # Function deployment configuration
    manage_functions.py           # Cloud Run function lifecycle CLI
    manage_infra.py               # BigQuery infrastructure provisioning CLI
  tests/
    fixtures/
      web_service_payload.json    # Sample Web Service task payload
    conftest.py                   # sys.path setup for test imports
    run_config.py                 # Quick config sanity check script
    test_bq_schemas.py            # Schema generation and consistency tests
    test_config.py                # Config loading and validation tests
    test_models.py                # Payload, participant, and QID_MAP tests
    test_validation.py            # Flask request parsing and extraction tests
```

## How it works

Qualtrics sends a completed survey response as a JSON POST via a Workflow Web Service task. The Cloud Run function (`main.py`) receives it, validates the payload against `WebServicePayload` (a Pydantic model with 34 typed fields), writes all fields as explicit BigQuery columns, then extracts and validates participant scheduling data.

The BigQuery schema is generated directly from the `WebServicePayload` model at import time (`bq_schemas.py`), so there is a single source of truth for field names and types. Two system columns (`created_at`, `processed`) are appended after the model-derived fields.

## CLI tools

Both scripts live in `gcp/deploy/` and are run from the project root.

### manage_functions.py -- Cloud Run function lifecycle

```bash
# Local development (starts functions-framework on port 8080)
python gcp/deploy/manage_functions.py dev run-qualtrics-scheduling

# Local development on a custom port
python gcp/deploy/manage_functions.py dev run-qualtrics-scheduling --port 9090

# Deploy to GCP
python gcp/deploy/manage_functions.py deploy run-qualtrics-scheduling

# Tear down (interactive confirmation)
python gcp/deploy/manage_functions.py teardown run-qualtrics-scheduling

# Tear down (skip confirmation)
python gcp/deploy/manage_functions.py teardown run-qualtrics-scheduling --force

# List all configured functions
python gcp/deploy/manage_functions.py list
```

The `dev` command copies `shared/` into the function directory, starts a local server, and cleans up on exit. It also prints any secrets the function expects so you can set them as environment variables (via `.envrc` / direnv).

The `deploy` command exports a `requirements.txt` from Poetry (using the function's dependency group), copies `shared/`, deploys via `gcloud functions deploy`, and cleans up local artifacts.

The `teardown` command deletes the Cloud Run function, removes leftover Artifact Registry container images, and clears the generated `requirements.txt`.

All function configuration lives in `gcp/deploy/functions.yaml`. To add a new function, add an entry under `functions:` with `source_dir`, `poetry_group`, `entry_point`, `trigger`, and optionally `secrets` and `allow_unauthenticated`.

### manage_infra.py -- BigQuery provisioning

```bash
# Check current state of dataset and tables
python gcp/deploy/manage_infra.py status

# Create dataset and all tables with defined schemas (idempotent)
python gcp/deploy/manage_infra.py setup

# Delete tables (preserves dataset, interactive confirmation)
python gcp/deploy/manage_infra.py teardown

# Delete tables (skip confirmation)
python gcp/deploy/manage_infra.py teardown --force
```

`manage_infra.py` reads dataset and table names from `gcp_config.yaml` (the same file the function uses at runtime), so there is no drift between what the script provisions and what the function writes to. Only tables with schemas registered in `TABLE_REGISTRY` (inside the script) are created by `setup`; unregistered tables appear in `status` as "no schema defined yet" and are skipped.

## Updating the schema

When a survey question changes or a new field is added, four files need updating. The BigQuery schema regenerates automatically from the Pydantic model, so there is no separate schema definition to maintain.

1. **`models/qualtrics.py`** -- Update `QID_MAP` if question IDs changed. Add, remove, or modify fields on `WebServicePayload`. Each field needs a type annotation and a `Field(...)` with a description.

2. **`gcp/deploy/manage_infra.py`** -- If adding an entirely new table, register it in `TABLE_REGISTRY` with its schema, partition field, cluster fields, and description. For changes to existing tables, the schema is picked up automatically from `bq_schemas.py`.

3. **`tests/fixtures/web_service_payload.json`** -- Update the fixture to match the new payload shape. Every field on `WebServicePayload` must be present with a valid value.

4. **`tests/test_models.py`** -- Update or add assertions for new/changed fields. If scale items changed, update the label sets in `test_positive_affect_labels`, `test_negative_affect_labels`, or `test_breach_violation_labels`.

After making changes, run the test suite to confirm everything is consistent:

```bash
poetry run pytest gcp/tests/ -v
```

Key things the tests catch after a schema change: QID_MAP names that do not match `WebServicePayload` fields, insert row keys that do not match the generated schema, non-serializable values, duplicate column names, and missing partition/cluster fields.

**BigQuery caveat:** The streaming API does not support in-place schema changes on existing tables. If you modify the schema, you need to tear down and recreate the table:

```bash
python gcp/deploy/manage_infra.py teardown --force
python gcp/deploy/manage_infra.py setup
```

## Testing locally with cURL

Start the local dev server and send the fixture payload:

```bash
# Terminal 1: start the server
python gcp/deploy/manage_functions.py dev run-qualtrics-scheduling

# Terminal 2: send a test payload
curl -X POST http://localhost:8080 \
  -H "Content-Type: application/json" \
  -d @gcp/tests/fixtures/web_service_payload.json
```

A successful response returns `{"status": "success", ...}` with a 200. Validation failures return a 400 with a descriptive error message.

## Running tests

```bash
# All tests
poetry run pytest gcp/tests/ -v
```

```bash
# All tests with coverage
poetry run pytest gcp/tests/ -v \
  --cov=gcp/cloud_run_functions/run_qualtrics_scheduling \
  --cov=gcp/shared/utils \
  --cov-report=term-missing
```

```bash
# Individual test files
poetry run pytest gcp/tests/test_models.py -v
poetry run pytest gcp/tests/test_bq_schemas.py -v
poetry run pytest gcp/tests/test_config.py -v
poetry run pytest gcp/tests/test_validation.py -v
```



Tests mock BigQuery calls and use Flask test request contexts, so no GCP credentials or network access are needed.

## Configuration

All runtime configuration lives in YAML files under `cloud_run_functions/run_qualtrics_scheduling/configs/`. The config loader (`config_loader.py`) merges all YAML files in the directory and validates the result against the `AppConfig` Pydantic model. Missing or invalid fields produce clear validation errors at startup.

**gcp_config.yaml** -- GCP project ID, BigQuery dataset/table names, Secret Manager references.

**qualtrics_config.yaml** -- Qualtrics API base URL and survey URL base.

Secrets (`QUALTRICS_API_KEY`, `QUALTRICS_WEBHOOK_SECRET`) are injected as environment variables by GCP Secret Manager at deploy time. For local development, set them via `.envrc` / direnv or export them manually.

## Dependencies

Dependencies are managed by Poetry with dependency groups:

- `main` -- Shared dependencies used by all functions (pydantic, PyYAML, requests).
- `fn-qualtrics-scheduling` -- Dependencies specific to this function (Flask, functions-framework, google-cloud-bigquery).
- `dev` -- Development tools (pytest, ruff).

At deploy time, `manage_functions.py` exports `main` + the function's group into a `requirements.txt` that Cloud Run uses to build the container. New functions get their own Poetry group and an entry in `functions.yaml`.